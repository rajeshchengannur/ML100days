{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMo2QBZ+nTcjxk1p7/JSgcC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshchengannur/ML100days/blob/main/2_1_FeatureEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering is the process of using Domain knowledge to extract feature from row data"
      ],
      "metadata": {
        "id": "D-plHkNZcPF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Transformation"
      ],
      "metadata": {
        "id": "0adydejqciQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   MIssing values imputation.\n",
        "Before training the model you should remove the missing data or fill the missing data.\n",
        "fill - mean, median\n",
        "2.   One hot encoding for categorical data . ( Because ML works only on numerical data)\n",
        "3.   Create Bins for Numerical values like Age . ( Like 0-10 child, 10-18 Teenage etc) \n",
        "4.   Outlier detection\n",
        "5.   Feature scaling\n",
        "Euclidian distance between two featues is very large and the higher value will dominate in the machine model.\n",
        "(Eg. Age and Salary.  when you take x2-x1 salary is higher. So we have scale the values). Use tecniques like min max scaling, standardization,normalization,mean abs scaling etc ( values between -1 to +1)\n",
        "\n"
      ],
      "metadata": {
        "id": "YtShdOcEdCpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Construction\n",
        "\n",
        "You are creating a new column altogether. based on intution\n",
        "\n",
        "For example take the case of titanic dataset. In this , therre are 2 columns siblings &Parent . You can join this together and make a column like family.\n",
        "Using family, you add up the values and generate a categorical values like\n",
        "1- Alone, 2-4 small family, >4 Large family"
      ],
      "metadata": {
        "id": "zBfVy2D1gQ-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Selection\n",
        "Take only the important feature from the dataset.\n",
        "Eg : mnist dataset, hand written nos are of 28 X 28 pixels. ie 784 feature.\n",
        "For that we remove the boundaries from this feature list , so that only useful features are selected."
      ],
      "metadata": {
        "id": "po1Gv8mdhgH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Extraction.\n",
        "Creating new feature completly programatically\n",
        "\n",
        "Fo example, House prediction , rooms and washroom feature, calculate squrefeet and do modelling.\n",
        "\n",
        "PCA is an famous feature extraction technique. In this give feature, rotate in high dimensional space and construct a new feature.\n",
        "PCA, LDA, TSME etc"
      ],
      "metadata": {
        "id": "QkEJHnlEih9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6O_PFdVbNKC"
      },
      "outputs": [],
      "source": []
    }
  ]
}